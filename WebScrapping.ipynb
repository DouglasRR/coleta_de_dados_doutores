{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivo\n",
    "    Cliente solicitou uma coleta de dados de todos os ginecologistas de MG dentro do site da Doctoralia. Foi solicitado Cidade, Nome e Telefone.\n",
    "    link: https://www.doctoralia.com.br/pesquisa?q=Ginecologista&loc=Belo+Horizonte&filters%5Bspecializations%5D%5B0%5D=36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anotações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manualmente foi feito um teste para ver quantas páginas tinham, como resultado, teve 82 páginas onde haviam perfis de doutores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notei que é melhor utilizar o BeautifulSoup do que o Selenium. Não encontrei no site um padrão fácil de coleta, logo, o WebScrapping foi feito com BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não encontrei um padrão para navegar entre os perfis, logo, eu criei uma função(coletar) onde extrai todos os links do site, extraindo todos os perfis que eu preciso navegar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como é extraido todos os links do site, encontrei um padrão nos links de perfis e criei uma função(filtrar) para filtrar os links encontrados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coloquei uma coluna(link) para caso o cliente queira verificar manualmente o perfil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Módulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Módulo de Requisições e Coleta.\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Manipulação de Dados.\n",
    "import pandas as pd\n",
    "\n",
    "# Expressão Regular usada no código.\n",
    "import re\n",
    "\n",
    "# Exclusão de arquivos auxiliares.\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coleta():\n",
    "\n",
    "    # Método contrutor.\n",
    "    def __init__(self):\n",
    "\n",
    "        # Formatação de dataframe para inserção dos dados.\n",
    "        self.dataframe = pd.DataFrame(columns = ['Cidade', 'Nome', 'Telefone', 'Link'])\n",
    "\n",
    "    # Função para extração de todos os links do site.\n",
    "    def coletar(self, pagina):\n",
    "\n",
    "        # A variável url foi desenvolvida para navegar nas páginas onde são encontrados os perfis.\n",
    "        url = 'https://www.doctoralia.com.br/pesquisa?q=Ginecologista&loc=Belo+Horizonte&filters%5Bspecializations%5D%5B0%5D=36&page=' + str(pagina)\n",
    "        response = requests.get(url)\n",
    "        content = response.content\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        links = soup.find_all('a')\n",
    "\n",
    "        # Criação de arquivo auxiliar. Arquivo da extração dos links.\n",
    "        arquivo = open('links.txt', 'w')\n",
    "\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            arquivo.write(str(href) + '\\n')\n",
    "\n",
    "        arquivo.close()\n",
    "\n",
    "    # Função para filtragem do conteúdo do arquivo auxiliar(links.txt), será extraído apenas links dos perfis.\n",
    "    def filtrar(self):\n",
    "\n",
    "        arquivo = open('links.txt', 'r')\n",
    "        lista = []\n",
    "\n",
    "        # Laço para filtragem dos links.\n",
    "        for linha in arquivo:\n",
    "            if 'ginecologista' in linha or 'clinicas/' in linha:\n",
    "                if 'reviews' not in linha:\n",
    "                    if linha not in lista:\n",
    "                        conteudo = linha\n",
    "                        if conteudo not in lista:\n",
    "                            lista.append(linha)\n",
    "\n",
    "        # Criação de arquivo auxiliar com filtragem dos links.\n",
    "        arquivo2 = open('links-processados.txt', 'w')\n",
    "\n",
    "        for x in lista:\n",
    "            linha = x\n",
    "            arquivo2.write(str(linha))\n",
    "\n",
    "        arquivo2.close()\n",
    "        arquivo.close()\n",
    "\n",
    "    # Função para estruturar o .txt para lista em python para facilitar na utilização dos links.\n",
    "    def listar_links(self):\n",
    "\n",
    "        arquivo = open('links-processados.txt', 'r')\n",
    "        lista = []\n",
    "\n",
    "        for linha in arquivo:\n",
    "            lista.append(linha)\n",
    "\n",
    "        return lista\n",
    "\n",
    "    # Função para coletar telefone, caso não seja encontrado ou não tenha, devolverá ponto de interrogação(\"?\").\n",
    "    def coletar_telefone(self, indice):\n",
    "\n",
    "        try:\n",
    "            lista = self.listar_links()\n",
    "            url = lista[indice].strip()\n",
    "            response = requests.get(url)\n",
    "            html = response.content\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            telefone_element = soup.find('a', {'data-patient-app-event-name': 'dp-call-phone'})\n",
    "            telefone = telefone_element.get_text(strip=True)\n",
    "\n",
    "            return telefone\n",
    "        \n",
    "        except:\n",
    "\n",
    "            return \"?\"\n",
    "        \n",
    "    # Função para coletar o nome. Caso seja perfil de doutor(ginecologista), a coleta é de uma forma, caso sejaa perfil de clínica, a coleta é diferente.\n",
    "    def coletar_nome(self, link):\n",
    "\n",
    "        # Se for doutor.\n",
    "        if \"ginecologista\" in link:\n",
    "\n",
    "            response = requests.get(link.strip())\n",
    "            html = response.content\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            elemento_span = soup.find('span', itemprop='name')\n",
    "            nome = elemento_span.get_text(strip=True)\n",
    "            return nome\n",
    "        \n",
    "        # Se for clínica.\n",
    "        else:\n",
    "            response = requests.get(link.strip())\n",
    "            html = response.content\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            elemento_h1 = soup.find('h1', class_='h3')\n",
    "            nome = elemento_h1.get_text(strip=True)\n",
    "            return nome\n",
    "\n",
    "    # Função para coletar cidade. Caso não seja encontrado ou não tenha, devolverá ponto de interrogação(\"?\").\n",
    "    def coletar_cidade(self, link):\n",
    "\n",
    "        try:\n",
    "\n",
    "            if 'ginecologista' in link:\n",
    "\n",
    "                response = requests.get(link.strip())\n",
    "                html = response.content\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                elementos = soup.find_all('span', {'class': ['city', 'province region']})\n",
    "                valores = [re.search(r'content=\"(.*?)\"', str(elemento)).group(1) for elemento in elementos]\n",
    "                juntas = []\n",
    "\n",
    "                for i in range(0, len(valores), 2):\n",
    "                    juntas.append(valores[i] + ', ' + valores[i+1])\n",
    "\n",
    "                if len(juntas) == 1:\n",
    "                    return juntas[0]\n",
    "                \n",
    "                else:\n",
    "                    endereco_completo = ' / '.join(juntas)\n",
    "                    enderecos_concatenados = [endereco_completo]\n",
    "\n",
    "                    return enderecos_concatenados[0]\n",
    "            else:\n",
    "\n",
    "                response = requests.get(link)\n",
    "                html = response.content\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                elemento_a = soup.find('a', class_='text-body font-weight-bold')\n",
    "                endereco = elemento_a.get_text(strip=True)\n",
    "                cidade = endereco.split(',')[0].strip()\n",
    "\n",
    "                return cidade\n",
    "\n",
    "        except:\n",
    "\n",
    "            return '?'\n",
    "    \n",
    "    # Função de armazenagem dos dados coletados.\n",
    "    def guardar_dados(self, cidade, nome, telefone, link):\n",
    "        \n",
    "        nova_linha = {'Cidade': cidade, 'Nome': nome, 'Telefone' : telefone, 'Link': link}\n",
    "        self.dataframe = self.dataframe.append(nova_linha, ignore_index=True)\n",
    "\n",
    "    # Função para status de coleta. Criei para saber o quanto o algoritmo coletou e o quantas páginas faltam.\n",
    "    def contagem(self, pagina, itens_coletados):\n",
    "\n",
    "        print(f'Página Atual: {pagina} --- Total de Coletas:{itens_coletados}')\n",
    "\n",
    "\n",
    "    # Função principal de funcionamento da coleta, armazenagem e exportação da coleta de dados.\n",
    "    def iniciar(self): \n",
    "\n",
    "        count = 0\n",
    "        coletados = 0\n",
    "\n",
    "        # Laço para navegar pelas 82 páginas.\n",
    "        while count < 82:\n",
    "\n",
    "            # Esse pedaço do código é para coletar, filtrar e estruturar links dos perfis.\n",
    "            self.coletar(count)\n",
    "            self.filtrar()\n",
    "            lista = self.listar_links()\n",
    "\n",
    "            # Laço para entrar em cada link e coletar os dados solicitados.\n",
    "            for linha in range(len(lista)):    \n",
    "\n",
    "                telefone = self.coletar_telefone(linha)\n",
    "                nome = self.coletar_nome(lista[linha])\n",
    "                link = lista[linha]\n",
    "                endereco = self.coletar_cidade(lista[linha])\n",
    "\n",
    "                self.guardar_dados(endereco, nome, telefone, link)\n",
    "                coletados += 1\n",
    "                self.contagem(count, coletados)\n",
    "\n",
    "            count += 1\n",
    "        \n",
    "        # Exclusão dos arquivos auxiliares.\n",
    "        os.remove('links.txt')\n",
    "        os.remove('links-processados.txt')\n",
    "        \n",
    "        # Exportação dos dados\n",
    "        self.dataframe.to_excel('dados.xlsx', index=True)\n",
    "\n",
    "# Iniciar coleta\n",
    "_ = Coleta()\n",
    "_.iniciar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
